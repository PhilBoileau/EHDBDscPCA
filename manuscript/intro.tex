Principal component analysis (PCA) is a well-known dimensionality reduction technique, widely used for data pre-processing and exploratory data analysis (EDA). Although popular for the interpretability of its results and ease of implementation, PCA's ability to extract signal from high-dimensional data is demonstrably unstable \cite{Shen2013,Johnstone2009}, in that its recovered results can vary widely with perturbations of the data \cite{yu2013}. What is more, PCA is often unable to reduce the dimensionality of the data in a contextually meaningful manner \cite{ringner2008,Abid2018}. Consequently, variants of PCA have been developed in attempts to remedy these severe issues, including, among many others, sparse PCA (SPCA) \cite{Zou2006}, which increases the interpretability and stability of the principal components in high dimensions by sparsifying the loadings, and contrastive PCA (cPCA) \cite{Abid2018}, which captures relevant information in the data by eliminating technical effects through comparison to a so-called background dataset. While SPCA and cPCA have both individually proven useful in resolving distinct shortcomings of PCA, neither is capable of simultaneously tackling the issues of interpretability, stability, and relevance. We propose a combination of these techniques, \textit{sparse constrastive PCA} (scPCA), which draws on cPCA to remove technical effects and on SPCA for sparsification of the loadings, thereby extracting interpretable, stable, and uncontaminated signal from high-dimensional biological data.

\subsection{Motivation}

A longstanding problem in genomics and related disciplines centers on teasing out important biological signal from technical noise, i.e., removing \textit{unwanted} variation corresponding to experimental artifacts (e.g., batch effects). A common preliminary approach for accomplishing such a task involves the application of classical PCA to capture and deflate technical noise, followed by traditional statistical inference techniques (e.g., clustering cells, testing for differences in mean gene expression levels between populations of cells) \cite{Nguyen2019}. Such an approach operates under the assumption that meaningful biological signal is not present in the leading principal components (PCs), and that the removal of the variance contained therein allows recovery of the signal previously masked by technical noise. Should these assumptions prove unmet, relevant biological signal may be unintentionally discarded, or worse, technical noise may be significantly amplified.

Several more sophisticated approaches have been proposed, including the use of control genes \cite{gagnon2012using, risso2014normalization} and control samples \cite{gagnon2013removing} whose behavior is known \textit{a priori}. Unfortunately, access to such controls may be severely limited in many settings (e.g., as with prohibitively expensive assays). Alternative approaches, for use in settings where control genes or control samples are unavailable, such as surrogate variable analysis \cite{leek2007capturing}, reconstruct sources of unwanted variation that may subsequently be controlled for via covariate adjustment in a typical regression modeling framework. In the context of single-cell RNA-seq data, a class of data that has garnered much interest due to the granularity of biological information it encodes, related approaches have been combined as part of the ZINB-WaVE methodology \cite{risso2017zinb}, which uses a strategy based on factor analysis to remove unwanted variation.

Although such approaches have proven useful, model-based techniques rely on assumptions about the data-generating process to target biological signal, warranting that much caution be taken in their use. Additionally, owing to the diversity of experimental settings in high-dimensional biology, such techniques are often targeted to specific experimental paradigms (e.g., bulk RNA-seq but \textit{not} single-cell RNA-seq). Violations of the assumptions embedded in these techniques may often be difficult --- impossible, even --- to diagnose, leading to a lack of overlap in findings between such model-based approaches when applied to the same datasets. Accordingly, \citet{zhang2014} have shown the lack of consensus among model-based differential expression techniques on RNA-seq datasets, demonstrating that their use gives rise to subjective analyses. By contrast, we propose a wholly data-driven approach to removing unwanted variation, harnessing the information contained in control samples, pre-treatment groups, or other signal-free observations, all while enhancing the interpretability and stability of findings by inducing sparsity.

The remainder of the present manuscript is organized as follows. In Section \ref{back}, contrastive PCA, sparse PCA, and other popular dimensionality reduction techniques are briefly surveyed. Next, in Section \ref{method}, scPCA is formally defined and its desirable properties are detailed. A simulation study and several analyses of publicly available  microarray gene expression, and single-cell transcriptome sequencing (scRNA-seq) data are presented in Section \ref{results}, and the analysis of a protein expression dataset is detailed in Section \ref{sup_mice}, providing a rich comparison of the proposed methodology to other popular techniques currently relied upon for the exploration of high-dimensional biological data. Finally, we conclude by reviewing the effectiveness of scPCA based on the results of these experiments and discussing paths for further investigation. 

\subsection{Background}\label{back}


\subsubsection{Contrastive PCA}\label{cpca}

The development of contrastive PCA was motivated by the need to detect and visualize variation in the data deemed most relevant to the scientific question of interest. Given a target dataset believed to contain biological signal(s) of interest and a similar background dataset believed to comprise only noise (i.e., unwanted variation), the cPCA algorithm returns a subspace of the target data that contains (a portion of) the variation absent from the background data \cite{Abid2018}. cPCA aims to identify the pertinent variation in the target data by \textit{contrasting} the covariance matrix of its features with that of the background data. For example, consider a scRNA-seq dataset whose samples are contaminated by a batch effect. Provided a collection of control samples subjected to the same batch effect, cPCA may be used to remove this unwanted technical noise (see Section \ref{sim_scRNA-seq}).

Algorithmically, cPCA is very similar to PCA. Consider a column-centered target dataset $\mathbf{X}_{n \times p}$ and a column-centered background dataset $\mathbf{Y}_{m \times p}$, where $n$ and $m$ denote, respectively, the number of target and background observations (e.g., cells) and $p$ denotes the number of features (e.g., genes). Define their empirical covariance matrices as $\mathbf{C_X}$ and $\mathbf{C_Y}$, and let $\mathbb{R}_{\text{unit}}^p=\{\mathbf{v} \in \mathbb{R}^p: \lVert\mathbf{v}\rVert_2 = 1\}$ be the set of unit vectors of length $p$. The variances along direction $\mathbf{v} \in \mathbb{R}_{\text{unit}}^p$ in the target and background datasets are represented by $\lambda_{\mathbf{X}}(\mathbf{v}) = \mathbf{v}^\top\mathbf{C_X}\mathbf{v}$ and $\lambda_{\mathbf{Y}}(\mathbf{v}) = \mathbf{v}^\top\mathbf{C_Y}\mathbf{v}$, respectively. The most contrastive direction $\mathbf{v}^{\star}_\gamma$ for some fixed contrastive parameter $\gamma \in \mathbb{R}^+$ is found by solving
\begin{equation}
    \begin{aligned}
  \mathbf{v}^{\star}_\gamma & = \argmax_{\mathbf{v} \in \mathbb{R}_{\text{unit}}^p}
  \lambda_{\mathbf{X}}(\mathbf{v})-
    \gamma \lambda_{\mathbf{Y}}(\mathbf{v}) \\
  & = \argmax_{\mathbf{v} \in \mathbb{R}_{\text{unit}}^p}
    \mathbf{v}^\top(\mathbf{C_X} - \gamma\mathbf{C_Y})\mathbf{v} \\
  & = \argmax_{\mathbf{v} \in \mathbb{R}_{\text{unit}}^p}
    \mathbf{v}^\top\mathbf{C}_{\gamma}\mathbf{v},
\end{aligned}
\end{equation}
where $\mathbf{C}_{\gamma} =  \mathbf{C_X} - \gamma\mathbf{C_Y}$ is the contrastive covariance matrix \cite{Abid2018}. cPCA can therefore be performed by computing the eigenvalue decomposition of $\mathbf{C}_{\gamma}$. The eigenvectors of $\mathbf{C}_{\gamma}$ are then used to map the target data to the \textit{contrastive} principal components (cPCs).

The contrastive parameter $\gamma$ quantifies the trade-off between each feature's variances in the target and background datasets. When $\gamma = 0$, cPCA reduces to PCA --- hence, the variance along $\lambda_{\mathbf{X}}(\mathbf{v})$ is maximized. On the other hand, as $\gamma \rightarrow \infty$, the variance in the background data dominates the variance in the target data such that only directions spanned by the background dataset are captured. This is akin to projecting the target dataset into the space spanned by directions in the background data and then performing PCA on this projection \cite{Abid2018}. The effect of the contrastive parameter is illustrated in fig.~\ref{fig:contrastive_par} for simulated data similar to those used by \citet{Abid2018}.

Although cPCA offers a novel approach for the removal of unwanted variation, it possesses some drawbacks. In particular, no rigorous framework exists for selecting the contrastive parameter $\gamma$ in order to achieve the optimal amount of contrast between the target and background data. Indeed, \citet{Abid2018}'s approach to selecting an appropriate $\gamma$ relies on visual inspection. Additionally, as with PCA, loading vectors may be highly variable and difficult to interpret in high dimensions since they represent linear combinations of all variables in the dataset. Relatedly, cPCs are not certifiably free of unwanted technical and biological effects, potentially obscuring relevant biological signal. This issue is only exacerbated as the dimension of the subspace orthogonal to the background data increases, jeopardizing the stability of the cPCs and enfeebling conclusions drawn from them.

\subsubsection{Sparse PCA}\label{spca}

In addition to being difficult to interpret, the PCs generated by applying PCA to high-dimensional data are generally unstable; that is they are
subject to major changes under minor perturbations of the data 
(we refer to \citet{johnstone2018pca} for a recent review). Luckily, an abundance of techniques for sparsifying PCA loadings have been developed to mitigate these issues; we direct the interested reader to \citet{ZouReview2018} for a recent review. Here, we consider the SPCA technique developed by \citet{Zou2006}. In contrast to standard PCA, SPCA generates interpretable and stable loadings in high dimensions, with most entries of the matrix being zero.

SPCA was born from the geometric interpretation of PCA, which reframes PCA as a regression problem. Given a matrix whose columns form an orthonormal basis $\mathbf{V}_{p \times k}$, the objective is to find the projection $\mathbf{P}_k = \mathbf{V}_{p \times k}\mathbf{V}_{p \times k}^\top$ producing the best linear manifold approximation of the data $\mathbf{X}_{n \times p}$. This is accomplished by minimizing the mean squared error:
\begin{equation}
\mathbf{V}_{p \times k}^{\star} = \argmin_{\mathbf{V}_{p \times k}}\sum_{i=1}^n \lVert x_i-\mathbf{V}_{p \times k} \mathbf{V}_{p \times k}^\top x_i \rVert_2^2,
\end{equation}
where $x_i$ is the $i^{\text{th}}$ row of $\mathbf{X}$ and $\mathbf{V}_{p \times k}^{\star}$ is exactly the loadings matrix of the first $k$ PCs \cite{ZouReview2018}. A sparse loadings matrix can be obtained by imposing an elastic net constraint on a modification of this objective function.

\citet{Zou2006} show that optimizing the following criterion provides loadings of the first $k$ sparse PCs of $\mathbf{X}$:
\begin{equation}\label{eq:spca}
\begin{aligned}
  (\mathbf{A}^\star_{p \times k}, \mathbf{B}^\star_{p \times k}) & =
    \argmin_{\mathbf{A}_{p \times k}, \mathbf{B}_{p \times k}}
    \sum_{i=1}^n \lVert x_i - \mathbf{A}_{p \times k}\mathbf{B}_{p \times k}^\top x_i \rVert_2^2 + \\
    & \quad \quad \quad \quad \quad \quad \quad \quad \lambda_0 \sum_{j=1}^k\beta_j^2 +
    \sum_{j=1}^k\lambda_{1,j} \lvert \beta_j \rvert \\
  & \quad \; \text{subject to }\mathbf{A}_{p \times k}^\top\mathbf{A}_{p \times k} = \mathbf{I}_{k \times k},
\end{aligned}
\end{equation}
where $\beta_j$ is the $j^{\text{th}}$ column of $\mathbf{B}_{p \times k}$ and where$\lambda_0$ and $\lambda_{1,j}$ are, respectively, the $\ell_2$ and $\ell_1$ penalty parameters for the non-normalized $j^{\text{th}}$  loading $\beta_j$; as in the original SPCA manuscript the $\ell_1$ penalty is allowed to be loading-specific \cite{Zou2006}. Then, the sparse loadings are the normalized versions of $\beta_j^\star$, i.e., the vectors $\beta_j^\star/\lVert\beta_j^\star\rVert_2$. \citet{Zou2006} also show that the full dataset need not be used to optimize the criterion; indeed, only the Gram matrix $\mathbf{X}_{n \times p}^\top\mathbf{X}_{n \times p}$ is required \cite{Zou2006}. 

Although SPCA provides a transparent and efficient method for the sparsification of PCA's loading matrices, and hence the generation of stable principal components in high dimensions, its development stopped short of providing means by which to identify the most relevant directions of variation in the data, presenting an obstacle to its efficacious use in biological data exploration and analysis. This motivates the development of exploratory methods that build upon the strengths of both SPCA and cPCA.

\subsubsection{Other Competing Methods}\label{others}

Other general methods frequently employed to reduce the dimensionality of high-dimensional biological data include t-distributed stochastic neighbor embedding (t-SNE) \cite{Maaten08visualizingdata} and uniform manifold approximation and projection (UMAP) \cite{McInnes2018} (e.g., \cite{Amir2013, Becht2019}). Unlike PCA, SPCA, and cPCA, both are nonlinear dimensionality reduction techniques --- that is, they do not enforce linear relationships between features. Such a relaxation permits the capturing of local nonlinear structures in the data that would otherwise go unnoticed, though neither approach guarantees that their low-dimensional embeddings reflect the global structure of the data. \citet{Becht2019} demonstrated the extreme computational efficiency exhibited by these techniques in their application to large datasets while \citet{Amir2013} and \citet{Becht2019} illustrated the stability of their findings, further increasing their popularity as methods of choice for EDA in computational biology. Yet, the flexibility and speed of t-SNE and UMAP come at a cost: these techniques are not endowed with the ease of interpretability of factor analysis methods. In lacking an interpretable link between the data's features and low-dimensional representation, their use as hypothesis-generating tools is restricted. Furthermore, like PCA, neither t-SNE nor UMAP have the ability to explicitly remove unwanted technical effects.

Though the dimensionality reduction methods discussed thus far can be applied to various kinds of high-dimensional biological data, still many others have been developed expressly for use with specific high-throughput assay biotechnologies. One such method, ZINB-WaVE, relies on a zero-inflated negative binomial model to better account for the count nature, zero inflation, and over-dispersion of scRNA-seq data, and has been shown to outperform less tailored techniques such as t-SNE \cite{risso2017zinb}. Unlike more general factor analysis methods (e.g., PCA), ZINB-WaVE takes advantage of the rich annotation metadata that are often available with scRNA-seq datasets to remove sources of unwanted variation, while preserving global biological signal \cite{risso2017zinb}. Analogous to PCA, the latent factors produced by ZINB-WaVE are not sparse. Technical and biological noise may remain after taking into account known and unknown sources of unwanted variation \cite{risso2017zinb}, potentially blurring any meaningful interpretation of latent factors. Other successful methods for reducing the dimensionality of scRNA-seq data, such as scVI \cite{Lopez2018}, have relied on the variational autoencoder framework to learn nonlinear structures in the data at hand and thereby infer values of latent variables. Further discussion of such techniques lies outside the scope of the present work on account of the dissimilarity to methods inspired by factor analysis, like SPCA, cPCA, and ZINB-WaVE, which are our focus.